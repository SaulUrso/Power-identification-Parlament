{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook regarding measurements with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datautils\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import RNNutils\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"./Dataset/power-gb-train.tsv\"\n",
    "DATA_DIR = \"./Dataset/\"\n",
    "RES_DIR = \"./Results/\"\n",
    "EMBED_DIR = \"./Embeddings/\"\n",
    "CHECK_DIR = \"./Checkpoints/\"\n",
    "DEVICE = datautils.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset (with k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary... 0\n",
      "Building vocabulary... 1\n",
      "Building vocabulary... 2\n",
      "Building vocabulary... 3\n",
      "Building vocabulary... 4\n",
      "Processing fold 1...\n",
      "Processing fold 1... validation set\n",
      "Processing fold 2...\n",
      "Processing fold 2... validation set\n",
      "Processing fold 3...\n",
      "Processing fold 3... validation set\n",
      "Processing fold 4...\n",
      "Processing fold 4... validation set\n",
      "Processing fold 5...\n",
      "Processing fold 5... validation set\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, _, _ = datautils.split_kfold_dataset(DATASET)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = torchtext.data.utils.get_tokenizer(\n",
    "    \"spacy\", language=\"en_core_web_sm\"\n",
    ")\n",
    "\n",
    "# build vocabulary\n",
    "en_vocab_list = []\n",
    "for idx, fold in enumerate(X_train):\n",
    "    print(\"Building vocabulary...\", idx)\n",
    "    fold_vocab = datautils.build_vocab(fold, tokenizer)\n",
    "    fold_vocab.set_default_index(fold_vocab[\"<unk>\"])\n",
    "    en_vocab_list.append(fold_vocab)\n",
    "\n",
    "# process all sets (train, val)\n",
    "\n",
    "for idx, (train_fold, val_fold) in enumerate(zip(X_train, X_val)):\n",
    "\n",
    "    # get vocabulary buildt on current training fold\n",
    "    fold_vocab = en_vocab_list[idx]\n",
    "\n",
    "    print(f\"Processing fold {idx+1}...\")\n",
    "    # process training fold\n",
    "    X_train[idx] = datautils.data_process(train_fold, fold_vocab, tokenizer)\n",
    "\n",
    "    print(f\"Processing fold {idx+1}... validation set\")\n",
    "    # process validation fold\n",
    "    X_val[idx] = datautils.data_process(val_fold, fold_vocab, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1...\n",
      "Processing fold 2...\n",
      "Processing fold 3...\n",
      "Processing fold 4...\n",
      "Processing fold 5...\n"
     ]
    }
   ],
   "source": [
    "for idx, (train_fold, train_target, val_fold, val_target) in enumerate(\n",
    "    zip(X_train, y_train, X_val, y_val)\n",
    "):\n",
    "\n",
    "    print(f\"Processing fold {idx+1}...\")\n",
    "    # create dataset\n",
    "    train_dataset = datautils.TextDataset(\n",
    "        train_fold, train_target, en_vocab_list[idx]\n",
    "    )\n",
    "    val_dataset = datautils.TextDataset(\n",
    "        val_fold, val_target, en_vocab_list[idx]\n",
    "    )\n",
    "\n",
    "    # save datasets\n",
    "    torch.save(train_dataset, f\"{DATA_DIR}train_dataset_{idx}.pt\")\n",
    "    torch.save(val_dataset, f\"{DATA_DIR}val_dataset_{idx}.pt\")\n",
    "\n",
    "\n",
    "# save vocabulary list using pickle\n",
    "\n",
    "with open(f\"{EMBED_DIR}vocab_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(en_vocab_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset (with hold-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(DATASET)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = torchtext.data.utils.get_tokenizer(\n",
    "    \"spacy\", language=\"en_core_web_sm\"\n",
    ")\n",
    "curr_vocab = datautils.build_vocab(X_train, tokenizer)\n",
    "curr_vocab.set_default_index(curr_vocab[\"<unk>\"])\n",
    "\n",
    "# process datasets\n",
    "X_train = datautils.data_process(X_train, curr_vocab, tokenizer)\n",
    "X_val = datautils.data_process(X_val, curr_vocab, tokenizer)\n",
    "\n",
    "# create dataset objects\n",
    "X_train = datautils.TextDataset(X_train, y_train, curr_vocab)\n",
    "X_val = datautils.TextDataset(X_val, y_val, curr_vocab)\n",
    "\n",
    "# save datasets\n",
    "torch.save(X_train, f\"{DATA_DIR}train_dataset.pt\")\n",
    "torch.save(X_val, f\"{DATA_DIR}val_dataset.pt\")\n",
    "\n",
    "# save vocabulary with pickle\n",
    "with open(f\"{EMBED_DIR}vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(curr_vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hold-out validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "train_dataset = torch.load(f\"{DATA_DIR}train_dataset.pt\")\n",
    "val_dataset = torch.load(f\"{DATA_DIR}val_dataset.pt\")\n",
    "\n",
    "# load the vocabulary\n",
    "with open(f\"{EMBED_DIR}vocab.pkl\", \"rb\") as f:\n",
    "    curr_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "CLIP = 1\n",
    "BATCH_SIZE = 2048 * 4\n",
    "EMBEDDING_DIM = 25\n",
    "HIDDEN_DIM = 25\n",
    "OUTPUT_DIM = 1\n",
    "INPUT_DIM = len(curr_vocab)\n",
    "PATIECE = 5\n",
    "CHECKPOINT_STEPS = 10\n",
    "\n",
    "learning_rate_list = [0.01]\n",
    "weight_decay_list = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNutils.RNN(\n",
    "    INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, output_dim=OUTPUT_DIM, device=DEVICE\n",
    ")\n",
    "train_iterator = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.generate_batch,\n",
    ")\n",
    "val_iterator = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=val_dataset.generate_batch,\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "min_loss = float(\"inf\")\n",
    "patience = PATIECE\n",
    "\n",
    "# initailize dataframe for results\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"optim\",\n",
    "        \"lr\",\n",
    "        \"weight_decay\",\n",
    "        \"epoch\",\n",
    "        \"train_loss\",\n",
    "        \"val_loss\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f1_score\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for lr, weight_decay in itertools.product(\n",
    "    learning_rate_list, weight_decay_list\n",
    "):\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.process_time()\n",
    "\n",
    "        print(\"Training...\")\n",
    "        train_loss = RNNutils.train_rnn(\n",
    "            model, train_iterator, optimizer, criterion, CLIP, #device=DEVICE\n",
    "        )\n",
    "        print(\"Evaluating...\")\n",
    "        valid_loss, precision, recall, f1_score = RNNutils.evaluate(\n",
    "            model, val_iterator, criterion,# device=DEVICE\n",
    "        )\n",
    "\n",
    "        end_time = time.process_time()\n",
    "\n",
    "        results = pd.concat(\n",
    "            [\n",
    "                results,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"optim\": \"Adam\",\n",
    "                        \"lr\": lr,\n",
    "                        \"weight_decay\": 0,\n",
    "                        \"epoch\": [epoch + 1],\n",
    "                        \"train_loss\": [train_loss],\n",
    "                        \"val_loss\": [valid_loss],\n",
    "                        \"precision\": [precision],\n",
    "                        \"recall\": [recall],\n",
    "                        \"f1_score\": [f1_score],\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # save model checkpoint every 10 epochs\n",
    "        if (epoch + 1) % CHECKPOINT_STEPS == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"loss\": valid_loss,\n",
    "                    \"patience\": patience,\n",
    "                },\n",
    "                f\"{RES_DIR}rnn_checkpoint_{epoch + 1}.pt\",\n",
    "            )\n",
    "            results.to_csv(f\"{RES_DIR}rnn_results-1-temp.csv\", index=False)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Time: {end_time-start_time:.2f}s\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.3f}\")\n",
    "        print(f\"\\t Val. Loss: {valid_loss:.3f}\")\n",
    "        print(\n",
    "            f\"\\t Val. Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1_score:.3f}\"\n",
    "        )\n",
    "\n",
    "        # early stopping\n",
    "        if valid_loss < min_loss:\n",
    "            min_loss = valid_loss\n",
    "            patience = PATIECE\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience == 0:\n",
    "            break\n",
    "\n",
    "results.to_csv(f\"{RES_DIR}rnn_results-1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optim</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.679495</td>\n",
       "      <td>0.677676</td>\n",
       "      <td>0.532281</td>\n",
       "      <td>0.516542</td>\n",
       "      <td>0.470575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703805</td>\n",
       "      <td>0.680974</td>\n",
       "      <td>0.519346</td>\n",
       "      <td>0.511607</td>\n",
       "      <td>0.475854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.669179</td>\n",
       "      <td>0.678746</td>\n",
       "      <td>0.539636</td>\n",
       "      <td>0.531443</td>\n",
       "      <td>0.518594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.633891</td>\n",
       "      <td>0.701004</td>\n",
       "      <td>0.553453</td>\n",
       "      <td>0.544928</td>\n",
       "      <td>0.536978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.587370</td>\n",
       "      <td>0.738696</td>\n",
       "      <td>0.545186</td>\n",
       "      <td>0.543852</td>\n",
       "      <td>0.543568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.612563</td>\n",
       "      <td>0.723990</td>\n",
       "      <td>0.550993</td>\n",
       "      <td>0.548768</td>\n",
       "      <td>0.548079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.655943</td>\n",
       "      <td>0.685922</td>\n",
       "      <td>0.550219</td>\n",
       "      <td>0.549095</td>\n",
       "      <td>0.549050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  optim     lr  weight_decay  epoch  train_loss  val_loss  precision  \\\n",
       "1  Adam  0.001             0      2    0.679495  0.677676   0.532281   \n",
       "0  Adam  0.001             0      1    0.703805  0.680974   0.519346   \n",
       "2  Adam  0.001             0      3    0.669179  0.678746   0.539636   \n",
       "4  Adam  0.001             0      5    0.633891  0.701004   0.553453   \n",
       "6  Adam  0.001             0      7    0.587370  0.738696   0.545186   \n",
       "5  Adam  0.001             0      6    0.612563  0.723990   0.550993   \n",
       "3  Adam  0.001             0      4    0.655943  0.685922   0.550219   \n",
       "\n",
       "     recall  f1_score  \n",
       "1  0.516542  0.470575  \n",
       "0  0.511607  0.475854  \n",
       "2  0.531443  0.518594  \n",
       "4  0.544928  0.536978  \n",
       "6  0.543852  0.543568  \n",
       "5  0.548768  0.548079  \n",
       "3  0.549095  0.549050  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(f\"{RES_DIR}rnn_results-1.csv\")\n",
    "results.sort_values(\"f1_score\", inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training k-fold (ignore for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets saved in previous step\n",
    "train_dataset_list = []\n",
    "val_dataset_list = []\n",
    "\n",
    "# using 5-fold cross validation\n",
    "for idx in range(5):\n",
    "    train_dataset_list.append(torch.load(f\"{DATA_DIR}train_dataset_{idx}.pt\"))\n",
    "    val_dataset_list.append(torch.load(f\"{DATA_DIR}val_dataset_{idx}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1...\n",
      "Batch 1...\n",
      "torch.Size([128, 2609])\n",
      "torch.Size([128, 1])\n",
      "[150, 665, 437, 113, 443, 1631, 352, 118, 602, 126, 108, 645, 1010, 151, 1335, 221, 1347, 2553, 1290, 1055, 980, 276, 836, 182, 156, 298, 151, 166, 1581, 379, 860, 94, 161, 308, 175, 116, 585, 865, 246, 587, 121, 2609, 215, 252, 283, 1001, 666, 743, 181, 95, 236, 659, 100, 992, 371, 183, 794, 775, 953, 670, 390, 647, 112, 134, 349, 668, 123, 341, 128, 793, 1730, 106, 99, 1076, 163, 785, 1295, 467, 428, 989, 233, 868, 738, 186, 452, 108, 700, 636, 550, 624, 792, 1019, 172, 223, 452, 645, 196, 118, 218, 2443, 164, 833, 924, 358, 984, 333, 2386, 239, 751, 141, 588, 1413, 649, 329, 1016, 462, 107, 137, 93, 795, 286, 589, 140, 105, 208, 108, 99, 1622]\n",
      "Processing fold 2...\n",
      "Batch 1...\n",
      "torch.Size([128, 2365])\n",
      "torch.Size([128, 1])\n",
      "[709, 462, 2070, 492, 1006, 245, 186, 120, 243, 141, 123, 349, 1690, 178, 153, 511, 618, 219, 1191, 106, 917, 2365, 303, 208, 163, 500, 185, 112, 847, 652, 321, 249, 188, 827, 203, 279, 614, 1389, 107, 1605, 457, 119, 508, 134, 144, 531, 367, 128, 275, 132, 1783, 96, 136, 1343, 1446, 1851, 199, 1187, 323, 821, 518, 103, 1452, 1050, 1524, 138, 209, 96, 116, 298, 606, 397, 138, 238, 146, 392, 639, 199, 173, 99, 1076, 203, 175, 559, 2283, 544, 313, 120, 140, 895, 969, 186, 262, 151, 177, 1380, 2365, 101, 513, 303, 156, 94, 230, 826, 553, 103, 1312, 1330, 440, 193, 1177, 314, 383, 462, 444, 663, 99, 773, 127, 520, 1943, 1167, 1054, 606, 282, 448, 105, 98]\n",
      "Processing fold 3...\n",
      "Batch 1...\n",
      "torch.Size([128, 2022])\n",
      "torch.Size([128, 1])\n",
      "[200, 116, 151, 343, 865, 747, 206, 127, 128, 94, 169, 155, 103, 436, 287, 108, 194, 475, 117, 2005, 186, 1274, 886, 613, 128, 1194, 1140, 148, 101, 1287, 605, 223, 338, 662, 686, 1439, 93, 246, 1070, 2022, 165, 691, 319, 979, 704, 944, 935, 1127, 397, 319, 215, 130, 186, 551, 262, 241, 1233, 507, 808, 110, 224, 1328, 174, 1254, 103, 139, 96, 157, 418, 758, 240, 660, 186, 93, 135, 93, 117, 1410, 544, 533, 147, 139, 853, 327, 1843, 1201, 489, 105, 182, 184, 1123, 1188, 123, 231, 694, 543, 642, 121, 804, 130, 316, 224, 714, 346, 935, 139, 877, 622, 148, 144, 103, 706, 309, 99, 108, 132, 721, 833, 137, 440, 145, 109, 553, 1299, 1084, 369, 1991, 163]\n",
      "Processing fold 4...\n",
      "Batch 1...\n",
      "torch.Size([128, 2851])\n",
      "torch.Size([128, 1])\n",
      "[1085, 108, 569, 2226, 856, 128, 158, 151, 149, 1141, 574, 154, 121, 159, 560, 103, 113, 110, 489, 120, 761, 200, 178, 950, 125, 2851, 755, 89, 1939, 523, 1707, 635, 116, 127, 93, 132, 379, 1328, 710, 602, 178, 475, 1278, 296, 914, 1623, 409, 767, 423, 1141, 145, 313, 169, 115, 116, 368, 134, 669, 613, 94, 142, 1621, 118, 760, 134, 163, 1324, 145, 304, 176, 291, 138, 934, 155, 232, 123, 530, 961, 750, 307, 186, 1071, 727, 106, 596, 283, 685, 1012, 736, 101, 1545, 264, 985, 654, 772, 2573, 170, 110, 2229, 109, 520, 101, 241, 559, 134, 1189, 146, 127, 139, 1292, 1904, 105, 410, 193, 742, 926, 261, 1228, 119, 129, 140, 1471, 841, 114, 509, 647, 954, 1480]\n",
      "Processing fold 5...\n",
      "Batch 1...\n",
      "torch.Size([128, 2489])\n",
      "torch.Size([128, 1])\n",
      "[303, 685, 111, 604, 534, 591, 278, 104, 99, 1409, 97, 940, 190, 193, 333, 197, 461, 2347, 799, 107, 139, 868, 117, 258, 108, 114, 224, 169, 132, 254, 260, 467, 463, 853, 215, 755, 1407, 171, 909, 221, 1522, 120, 801, 102, 143, 223, 223, 771, 1413, 130, 102, 2292, 865, 275, 134, 291, 135, 1018, 366, 129, 102, 337, 852, 439, 393, 753, 502, 125, 693, 231, 114, 106, 109, 1138, 132, 1269, 255, 956, 1669, 123, 110, 1355, 678, 106, 107, 1743, 408, 109, 1120, 396, 2288, 112, 1395, 107, 109, 184, 1370, 102, 182, 750, 1315, 211, 503, 101, 713, 105, 524, 2489, 689, 154, 949, 1238, 667, 1155, 151, 109, 110, 1695, 293, 108, 435, 1169, 105, 140, 201, 391, 907, 1005]\n"
     ]
    }
   ],
   "source": [
    "# iterate over each fold, using dataloader for minibatches\n",
    "\n",
    "for idx, (train_dataset, val_dataset) in enumerate(\n",
    "    zip(train_dataset_list, val_dataset_list)\n",
    "):\n",
    "    print(f\"Processing fold {idx+1}...\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        collate_fn=train_dataset.generate_batch,\n",
    "    )\n",
    "\n",
    "    # print(train_dataset[0])\n",
    "    # iterate on the dataloaders\n",
    "    for i, (X_batch, y_batch, x_len) in enumerate(train_loader):\n",
    "        print(f\"Batch {i+1}...\")\n",
    "        print(X_batch.shape)\n",
    "        print(y_batch.shape)\n",
    "        print(x_len)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
