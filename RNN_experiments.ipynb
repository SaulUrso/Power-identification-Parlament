{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook regarding measurements with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datautils\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import RNNutils\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"./Dataset/power-gb-train.tsv\"\n",
    "DATA_DIR = \"./Dataset/\"\n",
    "RES_DIR = \"./Results/\"\n",
    "EMBED_DIR = \"./Embeddings/\"\n",
    "CHECK_DIR = \"./Checkpoints/\"\n",
    "DEVICE = datautils.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset (with hold-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72276\n",
      "46491\n",
      "37987\n",
      "33080\n",
      "29729\n",
      "27245\n",
      "25353\n",
      "23816\n",
      "22555\n",
      "21529\n",
      "20552\n",
      "19712\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(DATASET)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = torchtext.data.utils.get_tokenizer(\n",
    "    \"spacy\", language=\"en_core_web_sm\"\n",
    ")\n",
    "\n",
    "# build vocabulary\n",
    "min_freq = 1\n",
    "curr_vocab = None\n",
    "while curr_vocab is None or len(curr_vocab) > 20000:\n",
    "    curr_vocab = datautils.build_vocab(X_train, tokenizer, min_freq=min_freq)\n",
    "    curr_vocab.set_default_index(curr_vocab[\"<unk>\"])\n",
    "    min_freq += 1\n",
    "    print(len(curr_vocab))\n",
    "\n",
    "# process datasets\n",
    "X_train = datautils.data_process(X_train, curr_vocab, tokenizer)\n",
    "X_val = datautils.data_process(X_val, curr_vocab, tokenizer)\n",
    "\n",
    "# create dataset objects\n",
    "X_train = datautils.TextDataset(X_train, y_train, curr_vocab)\n",
    "X_val = datautils.TextDataset(X_val, y_val, curr_vocab)\n",
    "\n",
    "# save datasets\n",
    "torch.save(X_train, f\"{DATA_DIR}train_dataset.pt\")\n",
    "torch.save(X_val, f\"{DATA_DIR}val_dataset.pt\")\n",
    "\n",
    "# save vocabulary with pickle\n",
    "with open(f\"{EMBED_DIR}vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(curr_vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hold-out validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "train_dataset = torch.load(f\"{DATA_DIR}train_dataset.pt\")\n",
    "val_dataset = torch.load(f\"{DATA_DIR}val_dataset.pt\")\n",
    "\n",
    "# load the vocabulary\n",
    "with open(f\"{EMBED_DIR}vocab.pkl\", \"rb\") as f:\n",
    "    curr_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "CLIP = 1\n",
    "BATCH_SIZE = 2048 * 4\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "INPUT_DIM = len(curr_vocab)\n",
    "PATIECE = 10\n",
    "CHECKPOINT_STEPS = 10\n",
    "\n",
    "# experiment number\n",
    "RES_NUM = 2\n",
    "\n",
    "learning_rate_list = [0.01]\n",
    "weight_decay_list = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNutils.BiLSTM(\n",
    "    INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, output_dim=OUTPUT_DIM, device=DEVICE\n",
    ")\n",
    "train_iterator = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.generate_batch,\n",
    ")\n",
    "val_iterator = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=val_dataset.generate_batch,\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "min_loss = float(\"inf\")\n",
    "patience = PATIECE\n",
    "\n",
    "# initailize dataframe for results\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"optim\",\n",
    "        \"lr\",\n",
    "        \"weight_decay\",\n",
    "        \"epoch\",\n",
    "        \"train_loss\",\n",
    "        \"val_loss\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f1_score\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for lr, weight_decay in itertools.product(\n",
    "    learning_rate_list, weight_decay_list\n",
    "):\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.process_time()\n",
    "\n",
    "        print(\"Training...\")\n",
    "        train_loss = RNNutils.train_rnn(\n",
    "            model,\n",
    "            train_iterator,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            CLIP,  # device=DEVICE\n",
    "        )\n",
    "        print(\"Evaluating...\")\n",
    "        valid_loss, precision, recall, f1_score = RNNutils.evaluate(\n",
    "            model,\n",
    "            val_iterator,\n",
    "            criterion,  # device=DEVICE\n",
    "        )\n",
    "\n",
    "        end_time = time.process_time()\n",
    "\n",
    "        results = pd.concat(\n",
    "            [\n",
    "                results,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"optim\": \"Adam\",\n",
    "                        \"lr\": lr,\n",
    "                        \"weight_decay\": 0,\n",
    "                        \"epoch\": [epoch + 1],\n",
    "                        \"train_loss\": [train_loss],\n",
    "                        \"val_loss\": [valid_loss],\n",
    "                        \"precision\": [precision],\n",
    "                        \"recall\": [recall],\n",
    "                        \"f1_score\": [f1_score],\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # save model checkpoint every 10 epochs\n",
    "        if (epoch + 1) % CHECKPOINT_STEPS == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"loss\": valid_loss,\n",
    "                    \"patience\": patience,\n",
    "                },\n",
    "                f\"{RES_DIR}rnn_checkpoint_{epoch + 1}.pt\",\n",
    "            )\n",
    "            results.to_csv(\n",
    "                f\"{RES_DIR}rnn_results-{RES_NUM}-temp.csv\", index=False\n",
    "            )\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Time: {end_time-start_time:.2f}s\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.3f}\")\n",
    "        print(f\"\\t Val. Loss: {valid_loss:.3f}\")\n",
    "        print(\n",
    "            f\"\\t Val. Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1_score:.3f}\"\n",
    "        )\n",
    "\n",
    "        # early stopping\n",
    "        if valid_loss < min_loss:\n",
    "            min_loss = valid_loss\n",
    "            patience = PATIECE\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience == 0:\n",
    "            break\n",
    "\n",
    "results.to_csv(f\"{RES_DIR}rnn_results-{RES_NUM}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optim</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.679495</td>\n",
       "      <td>0.677676</td>\n",
       "      <td>0.532281</td>\n",
       "      <td>0.516542</td>\n",
       "      <td>0.470575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703805</td>\n",
       "      <td>0.680974</td>\n",
       "      <td>0.519346</td>\n",
       "      <td>0.511607</td>\n",
       "      <td>0.475854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.669179</td>\n",
       "      <td>0.678746</td>\n",
       "      <td>0.539636</td>\n",
       "      <td>0.531443</td>\n",
       "      <td>0.518594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.633891</td>\n",
       "      <td>0.701004</td>\n",
       "      <td>0.553453</td>\n",
       "      <td>0.544928</td>\n",
       "      <td>0.536978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.587370</td>\n",
       "      <td>0.738696</td>\n",
       "      <td>0.545186</td>\n",
       "      <td>0.543852</td>\n",
       "      <td>0.543568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.612563</td>\n",
       "      <td>0.723990</td>\n",
       "      <td>0.550993</td>\n",
       "      <td>0.548768</td>\n",
       "      <td>0.548079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.655943</td>\n",
       "      <td>0.685922</td>\n",
       "      <td>0.550219</td>\n",
       "      <td>0.549095</td>\n",
       "      <td>0.549050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  optim     lr  weight_decay  epoch  train_loss  val_loss  precision  \\\n",
       "1  Adam  0.001             0      2    0.679495  0.677676   0.532281   \n",
       "0  Adam  0.001             0      1    0.703805  0.680974   0.519346   \n",
       "2  Adam  0.001             0      3    0.669179  0.678746   0.539636   \n",
       "4  Adam  0.001             0      5    0.633891  0.701004   0.553453   \n",
       "6  Adam  0.001             0      7    0.587370  0.738696   0.545186   \n",
       "5  Adam  0.001             0      6    0.612563  0.723990   0.550993   \n",
       "3  Adam  0.001             0      4    0.655943  0.685922   0.550219   \n",
       "\n",
       "     recall  f1_score  \n",
       "1  0.516542  0.470575  \n",
       "0  0.511607  0.475854  \n",
       "2  0.531443  0.518594  \n",
       "4  0.544928  0.536978  \n",
       "6  0.543852  0.543568  \n",
       "5  0.548768  0.548079  \n",
       "3  0.549095  0.549050  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(f\"{RES_DIR}rnn_results-{RES_NUM}.csv\")\n",
    "results.sort_values(\"f1_score\", inplace=True)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
