{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from gensim.utils import simple_preprocess\n",
    "from multiprocessing import Pool\n",
    "from preprocessing_utils import preprocess_document_for_count\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import datautils\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './Dataset/power-gb-train.tsv'\n",
    "RES_DIR = './Results/'\n",
    "EMBED_DIR = './Embeddings/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rare Words Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2 = spacy.load(\"en_core_web_md\")\n",
    "nlp3 = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "X_train, y_train, X_val, y_val, _, _ =datautils.split_holdout_dataset(PATH)\n",
    "\n",
    "\n",
    "def write_rare_words_to_file(rare_words, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for word in rare_words:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "def detect_rare_words(documents, num_processes=8):\n",
    "    # Run the preprocessing of the documents\n",
    "    with Pool(num_processes) as pool:\n",
    "        tokens_lists = pool.map(preprocess_document_for_count, documents)\n",
    "    \n",
    "    # Count the word frequency\n",
    "    word_freq = Counter(word for tokens in tokens_lists for word in tokens)\n",
    "    # Find the rare words\n",
    "    rare_words1 = set(word for word, freq in word_freq.items() if freq < 10)\n",
    "    rare_words2 = set(word for word, freq in word_freq.items() if freq < 100)\n",
    "    rare_words3 = set(word for word, freq in word_freq.items() if freq < 1000)\n",
    "    write_rare_words_to_file(rare_words1, 'rare_words10.txt')\n",
    "    write_rare_words_to_file(rare_words2, 'rare_words100.txt')\n",
    "    write_rare_words_to_file(rare_words3, 'rare_words1000.txt')\n",
    "    return word_freq\n",
    "\n",
    "detect_rare_words(X_train, num_processes=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the text in a list of tokens after preprocessing\n",
    "def preprocess_text(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = [token.text for token in doc if token.text.lower()]\n",
    "    return processed_tokens\n",
    "\n",
    "#Returns the dataset in a json format (list of lists of tokens)\n",
    "def preprocess_training_set(training_set, output_path, nlp):\n",
    "    preprocessed_training_set = []\n",
    "    i=0\n",
    "    for text in training_set:\n",
    "        preprocessed_text = preprocess_text(text, nlp)\n",
    "        preprocessed_training_set.append(preprocessed_text)\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(preprocessed_training_set, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "preprocess_training_set(X_train, 'preprocessed_training_setsm.json', nlp1)\n",
    "preprocess_training_set(X_train, 'preprocessed_training_setmd.json', nlp2)\n",
    "preprocess_training_set(X_train, 'preprocessed_training_setlg.json', nlp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_training_set(input_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        preprocessed_training_set = json.load(file)\n",
    "    return preprocessed_training_set\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def join_texts(preprocessed_training_set):\n",
    "    return [join_tokens(tokens) for tokens in preprocessed_training_set]\n",
    "\n",
    "def read_file_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        parole = [line.strip() for line in file]\n",
    "    return parole\n",
    "\n",
    "def remove_words(testo, word_to_exclude):\n",
    "    parole_testo = testo.split()\n",
    "    parole_filtrate = [parola for parola in parole_testo if parola in word_to_exclude]\n",
    "    testo_filtrato = ' '.join(parole_filtrate)\n",
    "    return testo_filtrato\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest on different pretrainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for pipe in [\"sm\",\"md\",\"lg\"]:\n",
    "    print(\"Analyzing pipe: \"+pipe)\n",
    "    input_path = 'preprocessed_training_set'+pipe+'.json'\n",
    "    nlp = spacy.load('en_core_web_'+pipe)\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    # Rebuilt the training set\n",
    "    preprocessed_training_set = load_preprocessed_training_set(input_path)\n",
    "    X_train = join_texts(preprocessed_training_set)\n",
    "\n",
    "    for tresh in [\"10\",\"100\",\"1000\"]:\n",
    "        print(\"Analyzing treshold: \"+tresh)\n",
    "        file_word_to_exclude = \"rare_words\"+tresh+\".txt\"\n",
    "        word_to_exclude = read_file_words(file_word_to_exclude)\n",
    "\n",
    "        # Vettore di testi\n",
    "        vettore_testi = X_train\n",
    "        X_train = [remove_words(testo, word_to_exclude) for testo in vettore_testi]\n",
    "        X_train = [remove_stopwords(text, stop_words) for text in X_train]\n",
    "\n",
    "        #TFIDF Vectorization\n",
    "        X_train, vectorizer = datautils.tf_idf_preprocessing(X_train)\n",
    "        _, y_train, X_val, y_val, _, _ =datautils.split_holdout_dataset(PATH)\n",
    "        X_val = vectorizer.transform(X_val)\n",
    "\n",
    "        hyperparameters = {\n",
    "            \"n_estimators\": [1000,2000],\n",
    "            \"max_depth\": [10,25,50],\n",
    "            \"min_samples_split\": [3,5],\n",
    "            \"min_samples_leaf\": [2,3],\n",
    "        }\n",
    "\n",
    "        param_grid = list(ParameterGrid(hyperparameters))\n",
    "\n",
    "        results = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"n_estimators\",\n",
    "                \"max_depth\",\n",
    "                \"min_samples_split\",\n",
    "                \"min_samples_leaf\",\n",
    "                \"precision\",\n",
    "                \"recall\",\n",
    "                \"fscore\",\n",
    "                \"p_train\",\n",
    "                \"r_train\",\n",
    "                \"f_train\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # train random forest\n",
    "        for par in param_grid:\n",
    "            print(par)\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=par[\"n_estimators\"],\n",
    "                max_depth=par[\"max_depth\"],\n",
    "                min_samples_split=par[\"min_samples_split\"],\n",
    "                min_samples_leaf=par[\"min_samples_leaf\"],\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            print(par)\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = rf.predict(X_val)\n",
    "\n",
    "            #compute metrics on training set\n",
    "            p_train, r_train, f_train,_ = precision_recall_fscore_support(y_train, rf.predict(X_train), average=\"macro\")\n",
    "            #compute metrics on test set\n",
    "            precision, recall, fscore,_ = precision_recall_fscore_support(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "            print(f\"VAL fscore: {fscore:.4f}, TRain fscore: {f_train:.4f}\")\n",
    "            results = pd.concat(\n",
    "                [\n",
    "                    results,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"n_estimators\": [par[\"n_estimators\"]],\n",
    "                            \"max_depth\": [par[\"max_depth\"]],\n",
    "                            \"min_samples_split\": [par[\"min_samples_split\"]],\n",
    "                            \"min_samples_leaf\": [par[\"min_samples_leaf\"]],\n",
    "                            \"precision\": [precision],\n",
    "                            \"recall\": [recall],\n",
    "                            \"fscore\": [fscore],\n",
    "                            \"p_train\": [p_train],\n",
    "                            \"r_train\": [r_train],\n",
    "                            \"f_train\": [f_train],\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        results.to_csv(RES_DIR + \"rfstop\"+tresh+pipe+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hltenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
