{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained embeddings and fine tuining\n",
    "\n",
    "In the first cell we look at the pretrained embeddings available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import numpy as np\n",
    "# Show all available models in gensim-data\n",
    "gensim.downloader.info()['models'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Here we download one of the 4 pretrained models that we used 'word2vec-ruscorpora-300', \n",
    "# 'word2vec-google-news-300'  'glove-twitter-200'  'glove-wiki-gigaword-300'\n",
    "google_model = gensim.downloader.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we define the path of the downloaded pretrained model, these paths represent the location of this embeddings in the machine were this notebook was executed. The paths have to be substituted with the correct location of the embeddings where this notebook is currently executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "EMBEDDING_FILE_GIGA= 'C:/Users/filip/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300/glove-wiki-gigaword-300.txt'\n",
    "EMBEDDING_FILE_GOOGLE = 'C:/Users/filip/gensim-data/word2vec-google-news-300/word2vec-google-news-300/GoogleNews-vectors-negative300.bin'\n",
    "EMBEDDING_FILE_TW = 'C:/Users/filip/gensim-data/glove-twitter-200/glove-twitter-200/glove-twitter-200.txt'\n",
    "EMBEDDING_FILE_RUSS = 'C:/Users/filip/gensim-data/word2vec-ruscorpora-300/word2vec-ruscorpora-300/word2vec-ruscorpora-300'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import fasttext\n",
    "from datautils import documents_vector\n",
    "from datautils import documents_vector_pre\n",
    "import datautils\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from datautils import documents_vector\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "PATH = './Dataset/power-gb-train.tsv'\n",
    "RES_DIR = './Results/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load the pretrained model\n",
    "google_model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE_GOOGLE, binary=True)\n",
    "#we split the dataset\n",
    "X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(PATH)\n",
    "\n",
    "tr_fold = list(map(simple_preprocess, X_train))\n",
    "val_fold = list(map(simple_preprocess, X_val))\n",
    "\n",
    "\n",
    "#pooling\n",
    "X_train1 = documents_vector_pre(tr_fold, google_model)\n",
    "X_va1l = documents_vector_pre(val_fold, google_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compute the word analogy for the pretrained model and we load the test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\"\n",
    "test_file = 'resources/questions-words.txt'\n",
    "questions = requests.get(url).content.decode()\n",
    "with open(test_file,mode='w',encoding='utf-8') as outputfile:\n",
    "    outputfile.write(questions)\n",
    "print(questions[:1000])\n",
    "\n",
    "#word analogy with dummy4unknown\n",
    "w2v_large_analogy_dummy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "#word analogy \n",
    "w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we take a look at the scores of word analogy\n",
    "w2v_large_analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we take a look at the scores of word analogy with dummy for unknown\n",
    "w2v_large_analogy_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we apply a grid search over the hyperparameters of the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "hyperparameters1 = {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.1, 1.0, 10.0, 100.0, 1000.0,1500.0],\n",
    "        \"solver\": [\"lbfgs\"],\n",
    "        \"max_iter\": [100, 200, 500,700],\n",
    "    }\n",
    "\n",
    "param_grid1 = list(ParameterGrid(hyperparameters1))\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    columns=[\"penalty\", \"C\", \"solver\", \"max_iter\", \"F1 Score\",\"Precision\",\"Recall\"]\n",
    ")\n",
    "\n",
    "for par1 in param_grid1:\n",
    "            \n",
    "        model = LogisticRegression(**par1)\n",
    "        model.fit(X_train1, y_train)\n",
    "\n",
    "        # Compute F1 score,Prediction and Recall on validation set\n",
    "        y_val_pred = model.predict(X_va1l)\n",
    "        f1_macro = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "        precision= precision_score(y_val,y_val_pred)\n",
    "        recall=recall_score(y_val,y_val_pred)\n",
    "\n",
    "        print(f\"Parameters: {par1}\")\n",
    "        print(f\"\\tF1 score: {f1_macro}\")\n",
    "        results_df = pd.concat(\n",
    "            [\n",
    "                results_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"penalty\": par1[\"penalty\"],\n",
    "                        \"C\": par1[\"C\"],\n",
    "                        \"solver\": par1[\"solver\"],\n",
    "                        \"max_iter\": par1[\"max_iter\"],\n",
    "                        \"F1 Score\": f1_macro,\n",
    "                        \"Precision\": precision,\n",
    "                        \"Recall\":recall\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "#change the name when we change the pretrained model\n",
    "results_df.to_csv(RES_DIR+f\"results-Logistic-google.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we defined a function for the fine tuning of a word2vec model using punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def w2v_tok(nome,dimensione,google_model):\n",
    "    #we download the tokenizer\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    #we define the word2vec model dimension the one of the pretrained emebedding\n",
    "    finetuned_model = Word2Vec(vector_size=dimensione, min_count=1)\n",
    "    #we split the dataset\n",
    "    X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(PATH)\n",
    "\n",
    "    #we use the tokenizer to tokenize our training set\n",
    "    tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in X_train]\n",
    "\n",
    "    #we buld the vocabulary\n",
    "    finetuned_model.build_vocab(tokenized_corpus)\n",
    "\n",
    "    #we take the word that are common between our model and the pretrained\n",
    "    intersecting_words = set(finetuned_model.wv.key_to_index.keys()) & set(google_model.key_to_index.keys())\n",
    "\n",
    "    # Update the embeddings of the fine-tuned model with pre-trained embeddings for intersecting words\n",
    "    for word in intersecting_words:\n",
    "        finetuned_model.wv[word] = google_model[word]\n",
    "\n",
    "    # Train the fine-tuned Word2Vec model\n",
    "    finetuned_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "    finetuned_model.wv.save(f\"./Embeddings/w2v-pretrained-tok-{nome}\")\n",
    "\n",
    "    tr_fold = list(map(simple_preprocess, X_train))\n",
    "    val_fold = list(map(simple_preprocess, X_val))\n",
    "\n",
    "    X_train1 = documents_vector(tr_fold, finetuned_model)\n",
    "    X_va1l = documents_vector(val_fold, finetuned_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we defined a function for the fine tuning of a fasttext model using punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def ftx_tok(nome,dimensione,google_model):\n",
    "    #we download the tokenizer\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    #we define the fasttext model dimension the one of the pretrained emebedding\n",
    "    finetuned_model = FastText(vector_size=dimensione, min_count=1)\n",
    "    #we split the dataset\n",
    "    X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(PATH)\n",
    "\n",
    "    #we tokenize our training set\n",
    "    tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in X_train]\n",
    "    #we build the vocabulary\n",
    "    finetuned_model.build_vocab(tokenized_corpus)\n",
    "    #we take the word that are common between our model and the pretrained\n",
    "    intersecting_words = set(finetuned_model.wv.key_to_index.keys()) & set(google_model.key_to_index.keys())\n",
    "\n",
    "    # Update the embeddings of the fine-tuned model with pre-trained embeddings for intersecting words\n",
    "    for word in intersecting_words:\n",
    "        finetuned_model.wv[word] = google_model[word]\n",
    "\n",
    "    # Train the fine-tuned Word2Vec model\n",
    "    finetuned_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "    finetuned_model.wv.save(f\"./Embeddings/ftx-pretrained-tok-{nome}\")\n",
    "\n",
    "    tr_fold = list(map(simple_preprocess, X_train))\n",
    "    val_fold = list(map(simple_preprocess, X_val))\n",
    "\n",
    "    #pooling\n",
    "    X_train1 = documents_vector(tr_fold, finetuned_model)\n",
    "    X_va1l = documents_vector(val_fold, finetuned_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below is defined the function that make the fine tuning using the bert based tokenizer. We fine tuned a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "def w2v_huggin(nome,dimensione,google_model):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    #we take the tokenier from bert\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    #we create a word2vec model\n",
    "    finetuned_model = Word2Vec(vector_size=dimensione, min_count=1)\n",
    "    #we split the dataset\n",
    "    X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(PATH)\n",
    "\n",
    "    #we tokenize\n",
    "    tokenized_corpus = [tokenizer.tokenize(sentence.lower()) for sentence in X_train]\n",
    "    #we build the vocabulary\n",
    "    finetuned_model.build_vocab(tokenized_corpus)\n",
    "\n",
    "    #we take the words that are common between our model vocabulartìy and the pretrained model\n",
    "    intersecting_words = set(finetuned_model.wv.key_to_index.keys()) & set(google_model.key_to_index.keys())\n",
    "\n",
    "    # Update the embeddings of the fine-tuned model with pre-trained embeddings for intersecting words\n",
    "    for word in intersecting_words:\n",
    "        finetuned_model.wv[word] = google_model[word]\n",
    "\n",
    "    # Train the fine-tuned Word2Vec model\n",
    "    finetuned_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "    #we save the model\n",
    "    finetuned_model.wv.save(f\"./Embeddings/w2v-pretrained-huggin-{nome}\")\n",
    "\n",
    "    tr_fold = list(map(simple_preprocess, X_train))\n",
    "    val_fold = list(map(simple_preprocess, X_val))\n",
    "\n",
    "    #we apply the pooling\n",
    "    X_train1 = documents_vector(tr_fold, finetuned_model)\n",
    "    X_va1l = documents_vector(val_fold, finetuned_model)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below is defined the function that make the fine tuning using the bert based tokenizer. We fine tuned a fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "def ftx_hug(nome,dimensione,google_model):\n",
    "\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    #we take the tokenier from bert\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    #we create a fasttext model\n",
    "    finetuned_model = FastText(vector_size=dimensione, min_count=1)\n",
    "    #we split the dataset in training and validation\n",
    "    X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(PATH)\n",
    "\n",
    "    #we tokenize\n",
    "    tokenized_corpus = [tokenizer.tokenize(sentence.lower()) for sentence in X_train]\n",
    "    #we build the vocabulary\n",
    "    finetuned_model.build_vocab(tokenized_corpus)\n",
    "\n",
    "    #we take the common word from our model and the pretrained model\n",
    "    intersecting_words = set(finetuned_model.wv.key_to_index.keys()) & set(google_model.key_to_index.keys())\n",
    "\n",
    "    # Update the embeddings of the fine-tuned model with pre-trained embeddings for intersecting words\n",
    "    for word in intersecting_words:\n",
    "        finetuned_model.wv[word] = google_model[word]\n",
    "\n",
    "    # Train the fine-tuned Fasttex model\n",
    "    finetuned_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "    #we save it\n",
    "    finetuned_model.wv.save(f\"./Embeddings/ftx-pretrained-huggin-{nome}\")\n",
    "\n",
    "    tr_fold = list(map(simple_preprocess, X_train))\n",
    "    val_fold = list(map(simple_preprocess, X_val))\n",
    "\n",
    "    #we procede with the pooling\n",
    "    X_train1 = documents_vector(tr_fold, finetuned_model)\n",
    "    X_va1l = documents_vector(val_fold, finetuned_model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took everyone of our models and we compute we apply the fine tuning with to our models(word2vec and fasttext) using 2 different tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vet=[EMBEDDING_FILE_GOOGLE,EMBEDDING_FILE_GIGA,EMBEDDING_FILE_TW,EMBEDDING_FILE_RUSS]\n",
    "for i in range(0,4):\n",
    "    \n",
    "    if(i==0 or i==3):\n",
    "        #here we put binary equals to true because 'google-news-300' and 'russ corpora 300'gives a binary file\n",
    "        google_model = KeyedVectors.load_word2vec_format(vet[i], binary=True)\n",
    "        if(i==0):\n",
    "            #fine tuning with google news 300\n",
    "            w2v_tok('google',300,google_model)\n",
    "            w2v_huggin('google',300,google_model)\n",
    "            ftx_hug('google',300,google_model)\n",
    "            ftx_tok('google',300,google_model)\n",
    "        else:\n",
    "            #fine tuning with russ corpora 300\n",
    "            w2v_tok('russ',300,google_model)\n",
    "            w2v_huggin('russ',300,google_model)\n",
    "            ftx_hug('russ',300,google_model)\n",
    "            ftx_tok('russ',300,google_model)\n",
    "    else:\n",
    "\n",
    "        #here we put binary equals to true because 'glove-twitter-200' and 'glove wiki gigaword 300'gives a binary file\n",
    "        google_model= KeyedVectors.load_word2vec_format(vet[i], binary=False)\n",
    "        if(i==2):\n",
    "            #fine tuning with twitter\n",
    "            w2v_tok('twitter',200,google_model)\n",
    "            w2v_huggin('twitter',200,google_model)\n",
    "            ftx_hug('twitter',200,google_model)\n",
    "            ftx_tok('twitter',200,google_model)\n",
    "        else:\n",
    "            #fine tuning with gigaword 300\n",
    "            w2v_tok('giga',300,google_model)\n",
    "            w2v_huggin('giga',300,google_model)\n",
    "            ftx_hug('giga',300,google_model)\n",
    "            ftx_tok('giga',300,google_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we evaluate on the validation set the fine tuned models. We apply a grid search over the possible hyperparameters of the logistic regression. We took only the fasttext fine tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#we take the the fasttext finte tuned model and we run them ove a logistic regression\n",
    "vet=['Embeddings/ftx-pretrained-tok-giga','Embeddings/ftx-pretrained-huggin-giga',\n",
    "     'Embeddings/ftx-pretrained-tok-twitter','Embeddings/ftx-pretrained-huggin-twitter',\n",
    "     'Embeddings/ftx-pretrained-tok-google','Embeddings/ftx-pretrained-huggin-google',\n",
    "     'Embeddings/ftx-pretrained-tok-russ','Embeddings/ftx-pretrained-huggin-russ']\n",
    "\n",
    "for i in range(0,8):\n",
    "    \n",
    "    #we load the fine tuned model\n",
    "    finetuned_model=model = KeyedVectors.load(vet[i])\n",
    "\n",
    "    # we split the data set into training and validation set\n",
    "    X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(PATH)\n",
    "\n",
    "\n",
    "    tr_fold = list(map(simple_preprocess, X_train))\n",
    "    val_fold = list(map(simple_preprocess, X_val))\n",
    "\n",
    "    #we apply the pooling\n",
    "    X_train1 = documents_vector_pre(tr_fold, finetuned_model)\n",
    "    X_va1l = documents_vector_pre(val_fold, finetuned_model)\n",
    "\n",
    "\n",
    "    #definition of the hyperparameters of the logistic regression\n",
    "    hyperparameters1 = {\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"C\": [0.1, 1.0, 10.0, 100.0, 1000.0,1500.0],\n",
    "            \"solver\": [\"lbfgs\"],\n",
    "            \"max_iter\": [100, 200, 500,700],\n",
    "        }\n",
    "\n",
    "    param_grid1 = list(ParameterGrid(hyperparameters1))\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"penalty\", \"C\", \"solver\", \"max_iter\", \"F1 Score\",\"Precision\",\"Recall\"]\n",
    "    )\n",
    "\n",
    "    for par1 in param_grid1:\n",
    "            #we create and train the logistic regression\n",
    "            model = LogisticRegression(**par1)\n",
    "            model.fit(X_train1, y_train)\n",
    "\n",
    "            # Compute F1 score, Precison and Recall on validation set\n",
    "            y_val_pred = model.predict(X_va1l)\n",
    "            f1_macro = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "            precision= precision_score(y_val,y_val_pred)\n",
    "            recall=recall_score(y_val,y_val_pred)\n",
    "\n",
    "            print(f\"Parameters: {par1}\")\n",
    "            print(f\"\\tF1 score: {f1_macro}\")\n",
    "            #we save everything in the dataframe\n",
    "            results_df = pd.concat(\n",
    "                [\n",
    "                    results_df,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"penalty\": par1[\"penalty\"],\n",
    "                            \"C\": par1[\"C\"],\n",
    "                            \"solver\": par1[\"solver\"],\n",
    "                            \"max_iter\": par1[\"max_iter\"],\n",
    "                            \"F1 Score\": f1_macro,\n",
    "                            \"Precision\": precision,\n",
    "                            \"Recall\":recall\n",
    "                            },\n",
    "                        index=[0],\n",
    "                        ),\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "    # we export the dataframe to csv\n",
    "    #for ponkt tokenizer\n",
    "    if(i==0):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-ftx-pretrained-tok-giga.csv\", index=False)\n",
    "    if(i==2):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-ftx-pretrained-tok-twitter.csv\", index=False)\n",
    "    if(i==4):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-ftx-pretrained-tok-google.csv\", index=False)\n",
    "    if(i==6):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-ftx-pretrained-tok-russ.csv\", index=False)\n",
    "    #for hugginface tokenizer\n",
    "    if(i==1):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-ftx-pretrained-huggin-giga.csv\", index=False)\n",
    "    if(i==3):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-ftx-pretrained-huggin-twitter.csv\", index=False)\n",
    "    if(i==5):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-ftx-pretrained-huggin-google.csv\", index=False)\n",
    "    if(i==7):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-ftx-pretrained-huggin-russ.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we evaluate on the validation set the fine tuned models. We apply a grid search over the possible hyperparameters of the logistic regression. We took only the word2vec fine tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#we take the the word2vec fine tuned model and we run them ove a logistic regression\n",
    "vet=['Embeddings/w2v-pretrained-tok-giga','Embeddings/w2v-pretrained-huggin-giga',\n",
    "     'Embeddings/w2v-pretrained-tok-twitter','Embeddings/w2v-pretrained-huggin-twitter',\n",
    "     'Embeddings/w2v-pretrained-tok-google','Embeddings/w2v-pretrained-huggin-google',\n",
    "     'Embeddings/w2v-pretrained-tok-russ','Embeddings/w2v-pretrained-huggin-russ']\n",
    "\n",
    "for i in range(0,8):\n",
    "    #we load the fine tuned model\n",
    "    finetuned_model=model = KeyedVectors.load(vet[i])\n",
    "\n",
    "    # we split the data set into training and validation set\n",
    "    X_train, y_train, X_val, y_val, _, _ = datautils.split_holdout_dataset(PATH)\n",
    "\n",
    "\n",
    "    tr_fold = list(map(simple_preprocess, X_train))\n",
    "    val_fold = list(map(simple_preprocess, X_val))\n",
    "\n",
    "    #we apply the pooling\n",
    "    X_train1 = documents_vector_pre(tr_fold, finetuned_model)\n",
    "    X_va1l = documents_vector_pre(val_fold, finetuned_model)\n",
    "\n",
    "\n",
    "    #definition of the hyperparameters of the logistic regression\n",
    "    hyperparameters1 = {\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"C\": [0.1, 1.0, 10.0, 100.0, 1000.0,1500.0],\n",
    "            \"solver\": [\"lbfgs\"],\n",
    "            \"max_iter\": [100, 200, 500,700],\n",
    "        }\n",
    "\n",
    "    param_grid1 = list(ParameterGrid(hyperparameters1))\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        columns=[\"penalty\", \"C\", \"solver\", \"max_iter\", \"F1 Score\",\"Precision\",\"Recall\"]\n",
    "    )\n",
    "\n",
    "    for par1 in param_grid1:\n",
    "            #we create and train the logistic regression\n",
    "            model = LogisticRegression(**par1)\n",
    "            model.fit(X_train1, y_train)\n",
    "\n",
    "            # Compute F1 score, Precison and Recall on validation set\n",
    "            y_val_pred = model.predict(X_va1l)\n",
    "            f1_macro = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "            precision= precision_score(y_val,y_val_pred)\n",
    "            recall=recall_score(y_val,y_val_pred)\n",
    "\n",
    "            print(f\"Parameters: {par1}\")\n",
    "            print(f\"\\tF1 score: {f1_macro}\")\n",
    "            #we save everything in the dataframe\n",
    "            results_df = pd.concat(\n",
    "                [\n",
    "                    results_df,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"penalty\": par1[\"penalty\"],\n",
    "                            \"C\": par1[\"C\"],\n",
    "                            \"solver\": par1[\"solver\"],\n",
    "                            \"max_iter\": par1[\"max_iter\"],\n",
    "                            \"F1 Score\": f1_macro,\n",
    "                            \"Precision\": precision,\n",
    "                            \"Recall\":recall\n",
    "                            },\n",
    "                        index=[0],\n",
    "                        ),\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "    # we export the dataframe to csv\n",
    "    if(i==0):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-w2v-pretrained-tok-giga.csv\", index=False)\n",
    "    if(i==2):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-w2v-pretrained-tok-twitter.csv\", index=False)\n",
    "    if(i==4):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-w2v-pretrained-tok-google.csv\", index=False)\n",
    "    if(i==6):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-w2v-pretrained-tok-russ.csv\", index=False)\n",
    "    #for hugginface tokenizer\n",
    "    if(i==1):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-w2v-pretrained-huggin-giga.csv\", index=False)\n",
    "    if(i==3):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-w2v-pretrained-huggin-twitter.csv\", index=False)\n",
    "    if(i==5):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-w2v-pretrained-huggin-google.csv\", index=False)\n",
    "    if(i==7):\n",
    "        results_df.to_csv(RES_DIR+f\"results-Logistic-w2v-pretrained-huggin-russ.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the word analogy for every fasttext fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we take the the fasttext fine tuned model and we compute the word analogy\n",
    "vet=['Embeddings/ftx-pretrained-tok-giga','Embeddings/ftx-pretrained-huggin-giga',\n",
    "     'Embeddings/ftx-pretrained-tok-twitter','Embeddings/ftx-pretrained-huggin-twitter',\n",
    "     'Embeddings/ftx-pretrained-tok-google','Embeddings/ftx-pretrained-huggin-google',\n",
    "     'Embeddings/ftx-pretrained-tok-russ','Embeddings/ftx-pretrained-huggin-russ']\n",
    "\n",
    "\n",
    "analogie = pd.DataFrame(\n",
    "    columns=[\"Model\", \"Accuracy\"]\n",
    ")\n",
    "\n",
    "for i in range(0,8):\n",
    "    \n",
    "    if(i==0):\n",
    "\n",
    "        #we load the model \n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        #we evaluate with word analogy\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        #we put the result on a dataframe\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Model\":'ftx-giga-tok-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        #we compute the word analogy with dummy for unknown\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "\n",
    "        #we crate the dataframe\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Model\":'ftx-giga-tok-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                    },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        \n",
    "    if(i==1):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-giga-hug-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        \n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-giga-hug-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        \n",
    "    if(i==2):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-twitter-tok-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-twitter-tok-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==3):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-twitter-hug-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-twitter-hug-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==4):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-google-tok-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-google-tok-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==5):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-google-hug-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-google-hug-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==6):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    { \n",
    "                        \"Model\":'ftx-russ-tok-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-russ-tok-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==7):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-russ-hug-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'ftx-russ-hug-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "analogie.to_csv(\"word_analogies_ftx.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the word analogy for every fasttext fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we take the the word2vec fine tuned model and we compute the word analogy\n",
    "vet=['Embeddings/w2v-pretrained-tok-giga','Embeddings/w2v-pretrained-huggin-giga',\n",
    "     'Embeddings/w2v-pretrained-tok-twitter','Embeddings/w2v-pretrained-huggin-twitter',\n",
    "     'Embeddings/w2v-pretrained-tok-google','Embeddings/w2v-pretrained-huggin-google',\n",
    "     'Embeddings/w2v-pretrained-tok-russ','Embeddings/w2v-pretrained-huggin-russ']\n",
    "\n",
    "\n",
    "analogie = pd.DataFrame(\n",
    "    columns=[\"Model\", \"Accuracy\"]\n",
    ")\n",
    "\n",
    "for i in range(0,8):\n",
    "    \n",
    "    if(i==0):\n",
    "\n",
    "        #we load the model as keyvector object\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        #we compute the word analogy\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        #we save the scores in a dataframe\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Model\":'w2v-giga-tok-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        #we compute the word analogy with dummy4unknown \n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Model\":'w2v-giga-tok-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                    },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        \n",
    "    if(i==1):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-giga-hug-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        \n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-giga-hug-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        \n",
    "    if(i==2):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-twitter-tok-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-twitter-tok-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==3):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-twitter-hug-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-twitter-hug-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==4):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-google-tok-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-google-tok-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==5):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-google-hug-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-google-hug-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==6):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    { \n",
    "                        \"Model\":'w2v-russ-tok-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-russ-tok-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    if(i==7):\n",
    "        google_model = KeyedVectors.load(vet[i])\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-russ-hug-false',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        w2v_large_analogy = google_model.evaluate_word_analogies(test_file,dummy4unknown=True)\n",
    "        analogie = pd.concat(\n",
    "            [\n",
    "                analogie,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                         \"Model\":'w2v-russ-hug-true',\n",
    "                        'Accuracy':w2v_large_analogy[0]\n",
    "                        },\n",
    "                    index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "analogie.to_csv(\"word_analogies_w2v.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
